# -*- coding: utf-8 -*-
"""projectIR.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ou10e1K9itLY8k34Ysh81HuJgAPHRP5l
"""

from transformers import AutoTokenizer, AutoModel
import torch

tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens')
model = AutoModel.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens')

import pandas as pd
df = pd.read_csv('test.csv')

df = df.dropna()
import csv
# Open the CSV file
with open('reviews.csv', 'r', encoding='utf-8') as csvfile:
    reader = csv.DictReader(csvfile)

    # Loop over each row in the CSV file
    for row in reader:
        # Extract the review text
        review = row['review']

        # Split the review text into words
        words = review.split()

        # Join the first 400 words into a new string
        new_review = ' '.join(words[:400])

        # Trim the new string to the last complete sentence
        if len(words) > 400:
            last_period_index = new_review.rfind('.')
            if last_period_index != -1:
                new_review = new_review[:last_period_index+1]

        # Update the row with the truncated review text
        row['review'] = new_review



df['review'] = df['review'].astype(str)
review_section = df['review']

# Remove meaningless semicolons and periods
preprocessed_reviews = []
for review in review_section:
    review = review.replace(';', '') # remove semicolons
    review = review.replace('. ', '.@@@') # temporarily replace valid periods with a unique delimiter
    review = review.replace('.', '') # remove any remaining periods
    review = review.replace('@@@', '. ') # replace the unique delimiter with valid periods
    preprocessed_reviews.append(review)

sentences= preprocessed_reviews
# # split the preprocessed reviews into sentences and store them in a list
# sentences = []
# for review in preprocessed_reviews:
#     sentences += review.split('. ')
    
# # remove any empty sentences from the list
# sentences = [s.strip() for s in sentences if s.strip()]


import torch
import faiss

batch_size = 32  # adjust as needed
input_ids = []
attention_mask = []

# Initialize a variable to accumulate the embeddings
mean_pooled_total = None
num_sentences = 0

for i in range(0, len(sentences), batch_size):
    batch = sentences[i:i+batch_size]
    new_tokens = tokenizer.batch_encode_plus(batch, max_length=512, truncation=True, padding='max_length', return_tensors='pt')
    input_ids.append(new_tokens['input_ids'])
    attention_mask.append(new_tokens['attention_mask'])

    with torch.no_grad():
        outputs = model(new_tokens['input_ids'], new_tokens['attention_mask'])

    embeddings = outputs.last_hidden_state
    attention_mask_batch = new_tokens['attention_mask'].unsqueeze(-1).expand_as(embeddings).float()
    masked_embeddings = embeddings * attention_mask_batch
    summed = torch.sum(masked_embeddings, 1)
    summed_mask = torch.clamp(attention_mask_batch.sum(1), min=1e-9)
    mean_pooled_batch = summed / summed_mask

    # Accumulate the embeddings
    if mean_pooled_total is None:
        mean_pooled_total = mean_pooled_batch.sum(dim=0)
    else:
        mean_pooled_total += mean_pooled_batch.sum(dim=0)

    num_sentences += len(batch)

    del new_tokens, embeddings, attention_mask_batch, masked_embeddings, summed, summed_mask, mean_pooled_batch

# Compute the final mean-pooled embeddings for all the sentences
mean_pooled = mean_pooled_total / num_sentences

index = faiss.IndexFlatIP(768)
print(index.is_trained)
index.add(mean_pooled.unsqueeze(0))
print(index.ntotal)
























# print(sentences)

# tokens={'input_ids' : [], 'attention_mask': []}
# for sentence in sentences:
#   new_tokens = tokenizer.encode_plus(sentence,max_length=512,truncation=True, padding='max_length', return_tensors='pt')
#   tokens['input_ids'].append(new_tokens['input_ids'][0])
#   tokens['attention_mask'].append(new_tokens['attention_mask'][0])

# tokens['input_ids']= torch.stack(tokens['input_ids'])
# tokens['attention_mask']= torch.stack(tokens['attention_mask'])


# import torch
# import faiss

# batch_size = 8
# n_samples = len(tokens['input_ids'])
# embeddings_list = []
# for i in range(0, n_samples, batch_size):
#     start = i
#     end = min(start+batch_size, n_samples)
#     batch_data = {'input_ids': tokens['input_ids'][start:end], 
#                   'attention_mask': tokens['attention_mask'][start:end]}
#     with torch.no_grad():
#         outputs = model(**batch_data)
    
#     embeddings = outputs.last_hidden_state
#     attention_mask = batch_data['attention_mask']
#     mask = attention_mask.unsqueeze(-1).expand_as(embeddings).float()
#     masked_embeddings = embeddings * mask
#     summed = torch.sum(masked_embeddings,1)
#     summed_mask = torch.clamp(mask.sum(1), min=1e-9)
#     mean_pooled = summed / summed_mask
#     embeddings_list.append(mean_pooled)

# embeddings = torch.cat(embeddings_list, dim=0)
# index = faiss.IndexFlatIP(768)
# index.add(embeddings.numpy())
# faiss.write_index(index, "test.index")


